{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MITLAB\\Miniconda3\\envs\\itri\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# %load helper.py\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.externals import joblib \n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVR, SVC\n",
    "from functools import partial\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "\n",
    "import json\n",
    "absFilePath = os.path.abspath(__file__)\n",
    "print('absFilePath'+absFilePath)\n",
    "fileDir = os.path.dirname(os.path.abspath(__file__))\n",
    "print('fileDir'+fileDir)\n",
    "parentDir = os.path.dirname(fileDir)\n",
    "parentDir = os.path.dirname(parentDir)\n",
    "print('parentDir'+parentDir)\n",
    "json_dir = parentDir+\"/results/json/\"\n",
    "image_dir = parentDir+\"/results/image/\"\n",
    "\n",
    "\n",
    "LL = ['LONGITUDE', 'LATITUDE']\n",
    "LLS = LL + ['SET']\n",
    "LLW = LL + ['WAP']\n",
    "LLSW = LLW + ['SET']\n",
    "\n",
    "def df_filter(df, s) :\n",
    "    filtered_df = df.loc[df['SET']==s, :].copy()\n",
    "    filtered_df.drop(['SET'],axis=1, inplace=True)\n",
    "    return filtered_df\n",
    "\n",
    "def get_hole(not_detected, WAP_number):\n",
    "    hole = not_detected.groupby(LL).count().reset_index()[LLW]\n",
    "    hole = hole[hole['WAP'] == WAP_number]\n",
    "    \n",
    "    hole = hole[LL].drop_duplicates()\n",
    "    hole['STATUS'] = 0 \n",
    "    hole['COVERAGE'] = 0 \n",
    "    hole = hole.set_index(LL)\n",
    "    return hole\n",
    "\n",
    "def get_poll(detected) :\n",
    "    poll = detected.groupby(LL).agg(['mean', 'count']).dropna().reset_index()\n",
    "    poll.columns = ['_'.join(tup).rstrip('_') for tup in poll.columns.values]\n",
    "    \n",
    "    poll = poll.rename({'WAP_count':'COVERAGE'}, axis=1)\n",
    "    poll['STATUS'] = 1\n",
    "    \n",
    "    threshold_poll = np.percentile(poll.SIGNAL_mean, 75) #origin is 75\n",
    "    poll['POLLUTION'] = poll.COVERAGE > 1# & (poll.SIGNAL_mean > threshold_poll)\n",
    "    poll.loc[poll.POLLUTION, 'STATUS'] = 2\n",
    "    \n",
    "    poll = poll[LL+['COVERAGE', 'STATUS']].drop_duplicates()\n",
    "    poll = poll.set_index(LL)\n",
    "    return poll\n",
    "\n",
    "def get_train_val(s, df_agg_max_all_set, r=0) :\n",
    "    df_agg_max = df_agg_max_all_set.loc[df_agg_max_all_set['SET']==s, :].copy()\n",
    "    detected_filter = df_agg_max['MIN_SIGNAL']>-1\n",
    "    # llw stands for longitude, Latitude, WAP\n",
    "    # means the locations are always covered by current WAP \n",
    "    df_detected_llw = df_agg_max[detected_filter].drop('MIN_SIGNAL', axis=1)\n",
    "    df_detected_llw = merge_agg(df_detected_llw, LLSW, 'SIGNAL', ['mean'])\n",
    "    df_detected_llw = df_detected_llw.drop('SIGNAL', axis=1).rename(columns={'mean' : 'SIGNAL'})\n",
    "    df_detected_llw = df_detected_llw.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # means the locations are not covered (once or more) by current WAP \n",
    "    df_not_detected_llw = df_agg_max[~detected_filter]\n",
    "    df_not_detected_llw = df_not_detected_llw.drop('SIGNAL', axis=1)\n",
    "    df_not_detected_llw = df_not_detected_llw.rename(columns={'MIN_SIGNAL' : 'SIGNAL'})\n",
    "    df_not_detected_llw = df_not_detected_llw.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    df_max = get_strongest(df_detected_llw)\n",
    "    \n",
    "    curr_WAP_source = df_filter(df_max, s)['WAP'].unique()\n",
    "    \n",
    "    all_data = df_filter(df_agg_max, s)\n",
    "    all_data = all_data[all_data.WAP.isin(curr_WAP_source)]\n",
    "    all_data = all_data.reset_index(drop=True)\n",
    "\n",
    "    detected = df_filter(df_detected_llw, s)\n",
    "    detected = detected[(detected.WAP.isin(curr_WAP_source))]\n",
    "    coverage = get_poll(detected)\n",
    "\n",
    "    not_detected = df_filter(df_not_detected_llw, s)\n",
    "    not_detected = not_detected[(not_detected.WAP.isin(curr_WAP_source))]\n",
    "    hole = get_hole(not_detected, len(curr_WAP_source))\n",
    "    coverage = coverage.append(hole)\n",
    "\n",
    "    coverage = coverage.reset_index()\n",
    "    \n",
    "    #print('status', np.unique(coverage.STATUS, return_counts=True))\n",
    "    #####-----PRINT JSON-----#####\n",
    "    json_data = {'status': str(np.unique(coverage.STATUS, return_counts=True))} \n",
    "    json_data = json.dumps(json_data)\n",
    "    with open(json_dir+'status.json', 'w') as fp:\n",
    "        fp.write(json_data)\n",
    "        \n",
    "    if len(coverage.STATUS == 2) <= 1:\n",
    "        pollute_happened = False\n",
    "        print('pollute didn\\'t happend.') \n",
    "        coverage = coverage[coverage['STATUS'] != 2]\n",
    "    else:\n",
    "        pollute_happened = True\n",
    "    \n",
    "    train_locs, val_locs = train_test_split(coverage, stratify=coverage.STATUS)\n",
    "    \n",
    "    #print('train locs', len(train_locs), 'val locs', len(val_locs))\n",
    "    #####-----PRINT JSON-----#####\n",
    "    json_data = {'train_locs': len(train_locs),\n",
    "                 'val_locs': len(val_locs)} \n",
    "    json_data = json.dumps(json_data)\n",
    "    with open(json_dir+'train_val_locs.json', 'w') as fp:\n",
    "        fp.write(json_data)\n",
    "        \n",
    "    train_locs = train_locs.reset_index(drop=True)\n",
    "    val_locs = val_locs.reset_index(drop=True)\n",
    "    \n",
    "    detected_train = detected.merge(train_locs, on=LL).drop_duplicates()\n",
    "    detected_val = detected.merge(val_locs, on=LL).drop_duplicates()\n",
    "    \n",
    "    not_detected_train = not_detected.merge(train_locs, on=LL).drop_duplicates()\n",
    "    not_detected_val = not_detected.merge(val_locs, on=LL).drop_duplicates()\n",
    "    \n",
    "    df_train = detected_train.append(not_detected_train)\n",
    "    df_train = shuffle(df_train, random_state=r)\n",
    "    \n",
    "    df_val = detected_val.append(not_detected_val)\n",
    "    df_val_generated = generate_polluted_val(df_val, curr_WAP_source)\n",
    "    df_val = shuffle(df_val, random_state=r)\n",
    "    \n",
    "    df_train = generate(df_train, df_max)\n",
    "    df_val = generate(df_val, df_max)\n",
    "    df_val_generated = generate(df_val_generated, df_max)\n",
    "    \n",
    "    #print('df_train', np.unique(df_train.STATUS, return_counts=True), \n",
    "    #      'df_val', np.unique(df_val.STATUS, return_counts=True))\n",
    "    #####-----PRINT JSON-----#####\n",
    "    json_data = {'df_train': str(np.unique(df_train.STATUS, return_counts=True)), \n",
    "                  'df_val': str(np.unique(df_val.STATUS, return_counts=True))} \n",
    "    json_data = json.dumps(json_data)\n",
    "    with open(json_dir+'df_train_val_locs.json', 'w') as fp:\n",
    "        fp.write(json_data)\n",
    "    \n",
    "    return train_locs, val_locs, df_train, df_val, df_val_generated, pollute_happened\n",
    "\n",
    "def generate_polluted_val(df_val_real, curr_WAP_source) :\n",
    "    df_val_generated = pd.DataFrame()\n",
    "    df_val_generated['LONGITUDE'] = df_val_real.LONGITUDE.repeat(len(curr_WAP_source))\n",
    "    df_val_generated['LATITUDE'] = df_val_real.LATITUDE.repeat(len(curr_WAP_source))\n",
    "    df_val_generated['WAP'] = np.tile(curr_WAP_source, len(df_val_real))\n",
    "    return df_val_generated\n",
    "\n",
    "def duplicate(df, n) :\n",
    "    df_result = pd.DataFrame()\n",
    "    for c in df.columns :\n",
    "        df_result[c] = df[c].repeat(n)\n",
    "    df_result = df_result.reset_index(drop=True)\n",
    "    return df_result\n",
    "\n",
    "def pairing(df_base, df_add) :\n",
    "    n = len(df_base)\n",
    "    df_base = duplicate(df_base, len(df_add))\n",
    "    df_base['LONGITUDE_SOURCE'] = pd.np.tile(df_add.LONGITUDE, n) \n",
    "    df_base['LATITUDE_SOURCE'] = pd.np.tile(df_add.LATITUDE, n) \n",
    "    df_base['SIGNAL_SOURCE'] = pd.np.tile(df_add.SIGNAL, n) \n",
    "    return df_base\n",
    "\n",
    "def merge_agg(df, group, value, aggregates, columns=None) :\n",
    "    df_count = pd.DataFrame(df.groupby(group)[value].agg(aggregates)).reset_index()\n",
    "    df_count.columns = group + aggregates if columns is None else group + columns\n",
    "    df = df.merge(df_count, on=group, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def restructure(df) :\n",
    "    df_final = pd.DataFrame()\n",
    "    for i in range(1,521) :\n",
    "        AP = 'WAP%03d' % i\n",
    "        df_temp = df[[AP]+LLBF]\n",
    "        df_temp = df_temp.rename(columns={AP : 'SIGNAL'})\n",
    "        df_temp['WAP'] = i\n",
    "        \n",
    "        df_final = df_final.append(df_temp, ignore_index=True)\n",
    "    df_final = df_final.drop_duplicates().reset_index(drop=True)\n",
    "    return df_final\n",
    "\n",
    "def get_strongest(df) :\n",
    "    df_max = pd.DataFrame()\n",
    "    for WAP in range(37, 43) :\n",
    "        df_temp = df[df['WAP'] == WAP].reset_index(drop=True)\n",
    "        max_val = df_temp['SIGNAL'].max()\n",
    "        \n",
    "        df_temp = df_temp[df_temp['SIGNAL'] == max_val]\n",
    "        df_temp = df_temp.drop_duplicates().reset_index(drop=True)\n",
    "        \n",
    "        df_max = df_max.append(df_temp).reset_index(drop=True)\n",
    "\n",
    "    return df_max\n",
    "    \n",
    "    \n",
    "def generate(df, df_max):\n",
    "    signal_exist = 'SIGNAL' in df.columns \n",
    "    curr_WAP_source = df_max['WAP'].unique()\n",
    "\n",
    "    df_final = pd.DataFrame()\n",
    "    for i in curr_WAP_source :\n",
    "        df_temp = df[df['WAP']==i]\n",
    "\n",
    "        #add pairing\n",
    "        curr_df_max_by_WAP = df_max[df_max['WAP']==i]\n",
    "        df_temp = pairing(df_temp, curr_df_max_by_WAP)\n",
    "        \n",
    "        if signal_exist:\n",
    "            df_temp['DIFF_SIGNAL'] = df_temp['SIGNAL_SOURCE'] - df_temp['SIGNAL']\n",
    "\n",
    "        df_final = df_final.append(df_temp, ignore_index=True)\n",
    "\n",
    "    return df_final\n",
    "\n",
    "def plot_data(train, val):\n",
    "    fig = plt.figure(figsize=(25,5))\n",
    "    plot_label(1, train.LONGITUDE, train.LATITUDE, train.STATUS, 'Train Data')\n",
    "    plot_label(2, val.LONGITUDE, val.LATITUDE, val.STATUS, 'Val Data')\n",
    "    plt.savefig(image_dir+'train_val.png')\n",
    "    plt.close()\n",
    "    #plt.show()\n",
    "\n",
    "def plot_label(i, x, y, status, title) :\n",
    "    positive = (status == 2) \n",
    "    normal = (status == 1) \n",
    "    negative = (status == 0)\n",
    "    \n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.scatter(x[positive], y[positive], c='b', s=35, label='polluted')\n",
    "    plt.scatter(x[normal], y[normal], c='g', s=35, label='normal')\n",
    "    plt.scatter(x[negative], y[negative], c='r', s=35, label='hole')\n",
    "    \n",
    "    plt.legend(loc=2, prop={'size': 20})\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.title(title, fontsize=20)\n",
    "\n",
    "def other_result(name, f_regressor, \n",
    "                 f_clf_coverage, \n",
    "                 f_clf_coverage_2, \n",
    "                 f_clf_poll, f_clf_hole,  \n",
    "                 df_train, df_val, \n",
    "                 df_train_per_locs, df_val_per_locs, pollution_flag, s) :\n",
    "    dir = parentDir+\"\\\\results\\\\model\"\n",
    "    #LLW\n",
    "    x_train = df_train[LL+['WAP']].values\n",
    "    x_val = df_val[LL+['WAP']].values\n",
    "\n",
    "    #signal LLW\n",
    "    f_regressor.fit(x_train, df_train['SIGNAL'])\n",
    "    joblib.dump(f_regressor, os.path.join(dir, name+\"_set\" + str(s) + \"_regressor_model.pth\")) ###\n",
    "    y_pred_signal = f_regressor.predict(x_val)    \n",
    "    mse = mean_squared_error(df_val['SIGNAL'], y_pred_signal)\n",
    "\n",
    "    #coverage LLW\n",
    "    y_train = df_train['SIGNAL'] == -1\n",
    "    y_val = df_val['SIGNAL'] == -1\n",
    "    f_clf_coverage.fit(x_train, y_train)\n",
    "    joblib.dump(f_clf_coverage, os.path.join(dir, name+\"_set\" + str(s) + \"_clf_coverage_model.pth\")) ###\n",
    "    y_pred_coverage = f_clf_coverage.predict(x_val)\n",
    "    c_acc = np.mean(y_val == y_pred_coverage)\n",
    "\n",
    "    #LL\n",
    "    x_train = df_train_per_locs[LL].values\n",
    "    x_val = df_val_per_locs[LL].values\n",
    "\n",
    "    #coverage LL\n",
    "    y_train = df_train_per_locs['COVERAGE']\n",
    "    y_val = df_val_per_locs['COVERAGE']\n",
    "    \n",
    "    f_clf_coverage_2.fit(x_train, y_train)\n",
    "    joblib.dump(f_clf_coverage_2, os.path.join(dir, name+\"_set\" + str(s) + \"_clf_coverage2_model.pth\")) ###\n",
    "    y_pred_coverage_ll = f_clf_coverage_2.predict(x_val)\n",
    "    c_mse = mean_squared_error(y_val, y_pred_coverage_ll)\n",
    "    \n",
    "    # pollution LL  \n",
    "    if pollution_flag:\n",
    "        y_train = df_train_per_locs['STATUS'] == 2\n",
    "        y_val = df_val_per_locs['STATUS'] == 2\n",
    "    \n",
    "        f_clf_poll.fit(x_train, y_train)\n",
    "        joblib.dump(f_clf_poll, os.path.join(dir, name+\"_set\" + str(s) + \"_clf_pollution.pth\")) ###\n",
    "        y_pred_poll = f_clf_poll.predict(x_val)\n",
    "        p_acc = np.mean(y_pred_poll == y_val)\n",
    "\n",
    "    # hole LL\n",
    "    y_train = df_train_per_locs['STATUS'] == 0\n",
    "    y_val = df_val_per_locs['STATUS'] == 0\n",
    "    \n",
    "    f_clf_hole.fit(x_train, y_train)\n",
    "    joblib.dump(f_clf_hole, os.path.join(dir, name+\"_set\" + str(s) + \"_clf_hole.pth\")) ###\n",
    "    y_pred_hole = f_clf_hole.predict(x_val)\n",
    "    h_acc = np.mean(y_pred_hole == y_val)\n",
    "    \n",
    "    if pollution_flag:\n",
    "        #print(name, 'mse signal LLW', mse, 'coverage LLW', c_acc) \n",
    "        #print('coverage LL', c_mse, 'acc polluted LL', p_acc, 'hole LL', h_acc)\n",
    "        #####-----PRINT JSON-----#####\n",
    "        json_data = {name: {'mse signal LLW': mse, \n",
    "                             'coverage LLW': c_acc,\n",
    "                            'coverage LL': c_mse,\n",
    "                             'acc polluted LL': p_acc,\n",
    "                             'hole LL': h_acc}}\n",
    "        json_data = json.dumps(json_data)\n",
    "        with open(json_dir+name+'.json', 'w') as f:\n",
    "            f.write(json_data)\n",
    "        \n",
    "        return y_pred_signal, y_pred_coverage, y_pred_coverage_ll, y_pred_poll, y_pred_hole\n",
    "    else:\n",
    "        #print(name, 'mse signal LLW', mse, 'coverage LLW', c_acc) \n",
    "        #print('coverage LL', c_mse, 'hole LL', h_acc)\n",
    "        #####-----PRINT JSON-----#####\n",
    "        json_data = {name: {'mse signal LLW': mse, \n",
    "                             'coverage LLW': c_acc,\n",
    "                            'coverage LL': c_mse,\n",
    "                             'hole LL': h_acc}}\n",
    "        json_data = json.dumps(json_data)\n",
    "        with open(json_dir+name+'.json', 'w') as f:\n",
    "            f.write(json_data)\n",
    "        return y_pred_signal, y_pred_coverage, y_pred_coverage_ll, y_pred_hole\n",
    "    \n",
    "\n",
    "def plot_encoder(i, preds, plot_type) :\n",
    "    _, df_ll = preds\n",
    "    x = df_ll['LONGITUDE']\n",
    "    y = df_ll['LATITUDE']\n",
    "    \n",
    "    #Coverage LL\n",
    "    if plot_type is 'coverage' :\n",
    "        diff = df_ll['COVERAGE'] - df_ll['pred_COVERAGE']\n",
    "        c_mse = mean_squared_error(df_ll['COVERAGE'], df_ll['pred_COVERAGE'])\n",
    "        plot(i, x, y, diff, 'encoder diff coverage')\n",
    "        #print('encoder mse', c_mse)\n",
    "        #####-----PRINT JSON-----#####\n",
    "    \n",
    "    #Pollution LL\n",
    "    elif plot_type is 'polluted' :\n",
    "        diff = (df_ll['STATUS']==2).astype(int) - df_ll['pred_POLLUTED'].astype(int)\n",
    "        p_acc = np.mean(((df_ll['STATUS']==2) == df_ll['pred_POLLUTED']))\n",
    "        plot_bool(i, x, y, (df_ll['STATUS']==2).astype(int), df_ll['pred_POLLUTED'].astype(int), 'encoder polluted')\n",
    "        #print('encoder acc', p_acc)\n",
    "        #####-----PRINT JSON-----#####\n",
    "    \n",
    "    #Hole LL\n",
    "    elif plot_type is 'hole' :\n",
    "        diff = (df_ll['STATUS']==0).astype(int) - df_ll['pred_HOLE'].astype(int)\n",
    "        h_acc = np.mean(((df_ll['STATUS']==0) == df_ll['pred_HOLE']))\n",
    "        plot_bool(i, x, y, (df_ll['STATUS']==0).astype(int), df_ll['pred_HOLE'].astype(int), 'encoder hole')\n",
    "        #print('encoder acc', h_acc)\n",
    "        #####-----PRINT JSON-----#####\n",
    "        \n",
    "    else :\n",
    "        print(plot_type, ' is not supported')\n",
    "\n",
    "def plot_other(i, df_val_per_locs, preds, name, pollution_flag, plot_type) :\n",
    "    if pollution_flag:\n",
    "        _, _, pred_coverage_ll, pred_poll, pred_hole = preds\n",
    "    else:\n",
    "        _, _, pred_coverage_ll, pred_hole = preds\n",
    "    x = df_val_per_locs['LONGITUDE']\n",
    "    y = df_val_per_locs['LATITUDE']\n",
    "    \n",
    "    #Coverage LL\n",
    "    if plot_type is 'coverage' :\n",
    "        diff = df_val_per_locs['COVERAGE'] - pred_coverage_ll\n",
    "        c_mse = mean_squared_error(df_val_per_locs['COVERAGE'], pred_coverage_ll)\n",
    "        plot(i, x, y, diff, name + ' diff coverage')\n",
    "        #print(name, 'mse', c_mse)\n",
    "        #####-----PRINT JSON-----#####\n",
    "    \n",
    "    #Pollution LL\n",
    "    elif plot_type is 'polluted' :\n",
    "        p_acc = np.mean((df_val_per_locs['STATUS']==2) == pred_poll)\n",
    "        plot_bool(i, x, y, (df_val_per_locs['STATUS']==2).astype(int), pred_poll, name + ' polluted')\n",
    "        #print(name, 'acc', p_acc)\n",
    "        #####-----PRINT JSON-----#####\n",
    "    \n",
    "    #Hole LL\n",
    "    elif plot_type is 'hole' :\n",
    "        h_acc = np.mean((df_val_per_locs['STATUS']==0) == pred_hole)\n",
    "        plot_bool(i, x, y, (df_val_per_locs['STATUS']==0).astype(int), pred_hole, name + ' hole')\n",
    "        #print(name, 'acc', h_acc)\n",
    "        #####-----PRINT JSON-----#####\n",
    "        \n",
    "    else :\n",
    "        print(plot_type, ' is not supported')\n",
    "    \n",
    "def plot(i, x, y, values, title) :\n",
    "    cm = plt.cm.get_cmap('RdYlGn_r')\n",
    "    cm = plt.cm.get_cmap('jet')\n",
    "    plt.subplot(2, 2, i)\n",
    "    sc = plt.scatter(x, y, c=values, s=50, cmap=cm)\n",
    "    sc = plt.colorbar(sc)\n",
    "    sc.ax.tick_params(labelsize=20)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.title(title, fontsize=20)\n",
    "    \n",
    "def plot_bool(i, x, y, values, preds, title) :\n",
    "    true_positive = (values == 1) & (preds == 1) \n",
    "    false_positive = (values == 0) & (preds == 1) \n",
    "    true_negative = (values == 0) & (preds == 0)\n",
    "    false_negative = (values == 1) & (preds == 0)\n",
    "    \n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.scatter(x[true_positive], y[true_positive], c='salmon', s=50, label='true positive')\n",
    "    plt.scatter(x[false_positive], y[false_positive], c='r', s=50, label='false positive')\n",
    "    plt.scatter(x[true_negative], y[true_negative], c='c', s=50, label='true negative')\n",
    "    plt.scatter(x[false_negative], y[false_negative], c='b', s=50, label='false negative')\n",
    "    \n",
    "    plt.legend(loc=2, prop={'size': 20})\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.title(title, fontsize=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}